import json
import re
import urllib
import urllib2
import urlparse

from BeautifulSoup import BeautifulSoup
from nanscrapers.common import clean_title, random_agent, replaceHTMLCodes
from ..scraper import Scraper


class Moviefree(Scraper):
    domains = ['hdmoviefree.org']
    name = "moviefree"

    def __init__(self):

        self.base_link = 'http://www.hdmoviefree.org'
        self.search_link = '/search/%s.html'
        self.server_link = '/ajax/loadsv/%s'
        self.episode_link = '/ajax/loadep/%s'

    def scrape_movie(self, title, year, imdb, debrid = False):
        try:
            headers = {'User-Agent': random_agent()}
            query = urlparse.urljoin(self.base_link, self.search_link % title.replace(' ', '-').replace('.', '-'))
            cleaned_title = clean_title(title)
            request = urllib2.Request(query, headers=headers)
            html = BeautifulSoup(urllib2.urlopen(request, timeout=30))
            posters = html.findAll('div', attrs={'class': 'two columns slideposter'})
            for poster in posters:
                try:
                    href = poster.findAll('a', attrs={'class': 'play-center tooltip'})[0]['href']
                    site_title = poster.findAll('h2', attrs={'class': 'nameonposter'})[0].text
                    if cleaned_title == clean_title(site_title):
                        return self.sources(replaceHTMLCodes(href))
                except:
                    continue
        except:
            pass
        return []

    def sources(self, url):
        try:
            sources = []

            if url == None: return sources
            url = urlparse.urljoin(self.base_link, url)
            headers = {'User-Agent': random_agent()}
            request = urllib2.Request(url, headers=headers)

            html = BeautifulSoup(urllib2.urlopen(request).read())

            images = html.findAll('img')

            data_id = None
            data_name = None

            for image in images:
                try:
                    data_id = image['data-id']
                    data_name = image['data-name']
                except:
                    continue

            if data_id is None or data_name is None:
                raise Exception()

            headers = {'X-Requested-With': 'XMLHttpRequest'}
            headers['Referer'] = url
            headers['User-Agent'] = random_agent()

            post = {'id': data_id, 'n': data_name}
            post = urllib.urlencode(post)
            url = self.server_link % data_id
            url = urlparse.urljoin(self.base_link, url)
            request = urllib2.Request(url, data=post, headers=headers)
            html = BeautifulSoup(urllib2.urlopen(request).read())

            links = html.findAll('a')

            for link in links:
                try:
                    try:
                        data_id = link['data-id']
                    except:
                        continue
                    url = self.episode_link % data_id
                    url = urlparse.urljoin(self.base_link, url)
                    post = {'epid': data_id}
                    post = urllib.urlencode(post)
                    request = urllib2.Request(url, data=post, headers=headers)
                    response = urllib2.urlopen(request).read()
                    source_json = json.loads(response)
                    try:
                        embedded_link = BeautifulSoup(source_json['link']['embed'])
                        u = embedded_link.findAll('iframe')[0]['src']
                    except:
                        u = source_json['link']['l']
                    for i in u:
                        try:
                            sources.append(
                                {'source': 'google video', 'quality': googletag(i)[0]['quality'], 'scraper': self.name,
                                 'url': i, 'direct': True})
                        except:
                            pass
                except:
                    pass


        except:
            pass
        return sources


def googletag(url):
    quality = re.compile('itag=(\d*)').findall(url)
    quality += re.compile('=m(\d*)$').findall(url)
    try:
        quality = quality[0]
    except:
        return []

    if quality in ['37', '137', '299', '96', '248', '303', '46']:
        return [{'quality': '1080', 'url': url}]
    elif quality in ['22', '84', '136', '298', '120', '95', '247', '302', '45', '102']:
        return [{'quality': '720', 'url': url}]
    elif quality in ['35', '44', '135', '244', '94']:
        return [{'quality': '480', 'url': url}]
    elif quality in ['18', '34', '43', '82', '100', '101', '134', '243', '93']:
        return [{'quality': '480', 'url': url}]
    elif quality in ['5', '6', '36', '83', '133', '242', '92', '132']:
        return [{'quality': '480', 'url': url}]
    else:
        return []